{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Optional, Callable, Any, Dict, Generator\n",
    "from itertools import islice\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import zarr\n",
    "import tifffile\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from torch.utils.data import DataLoader, IterableDataset, get_worker_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_path = '/localscratch/bsd_train.zarr/'\n",
    "reg_path = '/home/igor.zubarev/data/zarr_test/bsd_train.zarr'\n",
    "\n",
    "axes = 'SYX'\n",
    "patch_size = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr(\n",
    "    file_path: Path, axes: str\n",
    ") -> Union[zarr.core.Array, zarr.storage.DirectoryStore, zarr.hierarchy.Group]:\n",
    "    \"\"\"Reads a file and returns a pointer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : Path\n",
    "        pathlib.Path object containing a path to a file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Pointer to zarr storage\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError, OSError\n",
    "        if a file is not a valid tiff or damaged\n",
    "    ValueError\n",
    "        if data dimensions are not 2, 3 or 4\n",
    "    ValueError\n",
    "        if axes parameter from config is not consistent with data dimensions\n",
    "    \"\"\"\n",
    "    zarr_source = zarr.open(Path(file_path), mode=\"r\")\n",
    "    if isinstance(zarr_source, zarr.hierarchy.Group):\n",
    "        raise NotImplementedError(\"Group not supported yet\")\n",
    "\n",
    "    elif isinstance(zarr_source, zarr.storage.DirectoryStore):\n",
    "        raise NotImplementedError(\"DirectoryStore not supported yet\")\n",
    "\n",
    "    elif isinstance(zarr_source, zarr.core.Array):\n",
    "        # array should be of shape (S, (C), (Z), Y, X), iterating over S ?\n",
    "        # TODO what if array is not of that shape and/or chunks aren't defined and\n",
    "        if zarr_source.dtype == \"O\":\n",
    "            raise NotImplementedError(\"Object type not supported yet\")\n",
    "        else:\n",
    "            array = zarr_source\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported zarr object type {type(zarr_source)}\")\n",
    "\n",
    "    # TODO how to fix dimensions? Or just raise error?\n",
    "    # sanity check on dimensions\n",
    "    if len(array.shape) < 2 or len(array.shape) > 4:\n",
    "        raise ValueError(\n",
    "            f\"Incorrect data dimensions. Must be 2, 3 or 4 (got {array.shape}).\"\n",
    "        )\n",
    "\n",
    "    # sanity check on axes length\n",
    "    if len(axes) != len(array.shape):\n",
    "        raise ValueError(f\"Incorrect axes length (got {axes}).\")\n",
    "\n",
    "    # FIXME !\n",
    "    # arr = fix_axes(arr, axes)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_random(\n",
    "    arr: np.ndarray, patch_size: Union[List[int], Tuple[int]]\n",
    ") -> Generator[np.ndarray, None, None]:\n",
    "    \"\"\"\n",
    "    Generate patches from an array in a random manner.\n",
    "\n",
    "    The method calculates how many patches the image can be divided into and then\n",
    "    extracts an equal number of random patches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Input image array.\n",
    "    patch_size : Tuple[int]\n",
    "        Patch sizes in each dimension.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Generator[np.ndarray, None, None]\n",
    "        Generator of patches.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    n_patches_per_slice = np.ceil(np.prod(arr.shape[1:]) / np.prod(patch_size)).astype(\n",
    "        int\n",
    "    )\n",
    "    crop_coords = rng.integers(\n",
    "        0,\n",
    "        np.array(arr.shape[-len(patch_size) :]) - np.array(patch_size),\n",
    "        size=(arr.shape[0], n_patches_per_slice, len(patch_size)),\n",
    "    )\n",
    "    for slice_idx in range(crop_coords.shape[0]):\n",
    "        sample = arr[slice_idx]\n",
    "        for patch_idx in range(crop_coords.shape[1]):\n",
    "            patch = sample[\n",
    "                crop_coords[slice_idx, patch_idx, 0] : crop_coords[\n",
    "                    slice_idx, patch_idx, 0\n",
    "                ]\n",
    "                + patch_size[0],\n",
    "                crop_coords[slice_idx, patch_idx, 1] : crop_coords[\n",
    "                    slice_idx, patch_idx, 1\n",
    "                ]\n",
    "                + patch_size[1],\n",
    "            ]\n",
    "            yield patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZarrDataset(IterableDataset):\n",
    "    \"\"\"Dataset to extract patches from a zarr storage.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Union[str, Path],\n",
    "        axes: str,\n",
    "        patch_extraction_method: str,\n",
    "        patch_size: Optional[Union[List[int], Tuple[int]]] = None,\n",
    "        num_patches: Optional[int] = None,\n",
    "        mean: Optional[float] = None,\n",
    "        std: Optional[float] = None,\n",
    "        patch_transform: Optional[Callable] = None,\n",
    "        patch_transform_params: Optional[Dict] = None,\n",
    "    ) -> None:\n",
    "        self.data_path = Path(data_path)\n",
    "        self.axes = axes\n",
    "        self.patch_extraction_method = patch_extraction_method\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.patch_transform = patch_transform\n",
    "        self.patch_transform_params = patch_transform_params\n",
    "\n",
    "        self.sample = read_zarr(self.data_path, self.axes)\n",
    "\n",
    "    def _generate_patches(self):\n",
    "        patches = extract_patches_random(\n",
    "            self.sample,\n",
    "            self.patch_size,\n",
    "        )\n",
    "\n",
    "        for idx, patch in enumerate(patches):\n",
    "\n",
    "            if isinstance(patch, tuple):\n",
    "                patch = (patch, *patch[1:])\n",
    "            else:\n",
    "                patch = patch\n",
    "\n",
    "            if self.patch_transform is not None:\n",
    "                assert self.patch_transform_params is not None\n",
    "                patch = self.patch_transform(patch, **self.patch_transform_params)\n",
    "            if self.num_patches is not None and idx >= self.num_patches:\n",
    "                return\n",
    "            else:\n",
    "                yield patch\n",
    "        self.mean = self.running_stats.avg_mean\n",
    "        self.std = self.running_stats.avg_std\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate over data source and yield single patch.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        np.ndarray\n",
    "        \"\"\"\n",
    "        worker_info = get_worker_info()\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        yield from islice(self._generate_patches(), 0, None, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ZarrDataset(\n",
    "                data_path=reg_path,\n",
    "                axes=axes,\n",
    "                patch_extraction_method='random',\n",
    "                patch_size=patch_size,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=32, num_workers=4, prefetch_factor=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 64, 64])\n",
      "1 torch.Size([32, 64, 64])\n",
      "2 torch.Size([32, 64, 64])\n",
      "3 torch.Size([32, 64, 64])\n",
      "4 torch.Size([32, 64, 64])\n",
      "5 torch.Size([32, 64, 64])\n",
      "6 torch.Size([32, 64, 64])\n",
      "7 torch.Size([32, 64, 64])\n",
      "8 torch.Size([32, 64, 64])\n",
      "9 torch.Size([32, 64, 64])\n",
      "10 torch.Size([32, 64, 64])\n",
      "11 torch.Size([32, 64, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dl):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i, batch\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/localscratch/mambaforge/envs/cmls/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dl):\n",
    "    print(i, batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('HDNn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "faf8b084d52efbff00ddf863c4fb0ca7a3b023f9f18590a5b65c31dc02d793e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
